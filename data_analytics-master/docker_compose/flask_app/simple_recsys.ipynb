{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рекомендательная система в Docker: Postgres+Mongo+Flask\n",
    "\n",
    "В рамках данного воркшопа будет продемонстрирована архитектура хранения и обработки данных:\n",
    "\n",
    "- Использование реляционныой БД для хранения данных\n",
    "- Выгрузка и преварительная обработка данных с помощью SQL\n",
    "- Взаимодействие между БД и Python\n",
    "- Нереляционные хранилища: Mongo, Redis\n",
    "\n",
    "## Решение по хранению и обработке данных\n",
    "\n",
    "Запускаем Docker-контейнер с данными в PostgreSQL\n",
    "\n",
    "<pre>\n",
    "docker-compose --project-name data-cli -f docker-compose.yml up --build flask-app\n",
    "</pre>\n",
    "\n",
    "Подробнее про Docker тут: https://hackernoon.com/docker-tutorial-getting-started-with-python-redis-and-nginx-81a9d740d091\n",
    "\n",
    "Сборка контейнера включает в себя создание реляционной БД PostgreSQL с двумя таблицами (данные берём из CSV). В базу данных будут загружены файлы из [этого](https://www.kaggle.com/rounakbanik/the-movies-dataset/data) соревнования- их нужно скачать заранее, прягодятся для домашних работ.\n",
    "\n",
    "Если всё прошло успешно, то по url http://0.0.0.0:5001 можно будет увидеть приветственную страницу приложения\n",
    "\n",
    "![Главная страница приложения](https://habrastorage.org/webt/oc/rb/op/ocrbophkojh8ll_qsqp5naf3i-g.jpeg)\n",
    "\n",
    "Управлять выдачей на странице [\"SVD рекомендации\"](http://0.0.0.0:5001/recs?user_id=10&top=15) можно с помощью параметров user_id и top.\n",
    "\n",
    "![Страница приложения с рекомендациями](https://habrastorage.org/webt/bp/d9/q-/bpd9q-c1v1_k2kbrgxwvssgq0da.jpeg)\n",
    "\n",
    "## Получаем данные\n",
    "\n",
    "Данные хранятся в PostgreSQL - реляционной БД с открытым исходным кодом. Первая часть курса посвящена реляционным базам данных и языку SQL.\n",
    "\n",
    "Мы научимся писать сложные SQL запросы для фильтрации данных и предварительной обработки: удалению шумов, расчёта фичей и т.д.\n",
    "\n",
    "Язык запросов SQL позволяет строить гибкие пайплайны по обработке данных. Средства более высокого уровня для обработки \"сырых\" данных - например, Apache Spark - используют для построения процессов обработки примитивы, аналогичные SQL.\n",
    "\n",
    "Приведённый ниже сниппет демонтрирует ещё одно достоинство Postgres: в Python существует коннектор к этой БД, билиотека psycopg2. Таким образом можно отладить запрос в любой удобной для себя среде, а потом перенести его в Python-приложение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix\n",
    "import numpy as np\n",
    "\n",
    "# параметры подключения к БД\n",
    "params = {\"host\": \"localhost\", \"port\": 5433, \"user\": 'postgres'}\n",
    "conn = psycopg2.connect(**params)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# параметры SQL-запроса\n",
    "USER_ITEM_QUERY_CONFIG = {\n",
    "       \"MIN_USERS_FOR_ITEM\": 10,\n",
    "       \"MIN_ITEMS_FOR_USER\": 3,\n",
    "       \"MAX_ITEMS_FOR_USER\": 50,\n",
    "       \"MAX_ROW_NUMBER\": 100000\n",
    "}\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT \n",
    "        ratings.userId, ratings.movieId, AVG(ratings.rating) as rating\n",
    "    FROM ratings\n",
    "    -- фильтруем фильмы, которые редко оценивают\n",
    "    INNER JOIN (\n",
    "        SELECT \n",
    "            movieId, count(*) as users_per_item\n",
    "        FROM ratings \n",
    "        GROUP BY movieId \n",
    "        HAVING COUNT(*) > %(MIN_USERS_FOR_ITEM)d\n",
    "    ) as movie_agg\n",
    "        ON movie_agg.movieId = ratings.movieId\n",
    "    -- фильтруем пользователей, у которых мало рейтингов\n",
    "    INNER JOIN (\n",
    "        SELECT \n",
    "            userId, count(*) as items_per_user\n",
    "        FROM ratings \n",
    "        GROUP BY userId \n",
    "        HAVING COUNT(*) BETWEEN %(MIN_ITEMS_FOR_USER)d AND %(MAX_ITEMS_FOR_USER)d \n",
    "    ) as user_agg\n",
    "        ON user_agg.userId = ratings.userId\n",
    "    GROUP BY 1,2\n",
    "    LIMIT %(MAX_ROW_NUMBER)d\n",
    "\"\"\" % USER_ITEM_QUERY_CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закрываем соединение к БД и выгружаем данные в объект Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88736, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>movieid</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>147</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>858</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1221</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1246</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid  movieid  rating\n",
       "0       1      110     1.0\n",
       "1       1      147     4.5\n",
       "2       1      858     5.0\n",
       "3       1     1221     5.0\n",
       "4       1     1246     5.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вытаскиваем результаты SQL в память Python\n",
    "ui_data = [a for a in cursor.fetchall()]\n",
    "\n",
    "df = pd.DataFrame(ui_data, columns=[a.name for a in cursor.description])\n",
    "\n",
    "conn.close()\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг данных: переиндексация\n",
    "\n",
    "Для построения рекомендательной системы мы будем использовать алгоритм матричного разложения SVD.\n",
    "\n",
    "Для этого нужно сформировать из триплетов *[userid,\tmovieid, rating]* матрицу user-item ( подробнее в статье в блоге ivi https://habr.com/company/ivi/blog/232843/ ) . В матрице user-item число строк совпадает с числом уникальных пользователей, а число столбцов - с количеством единиц контента (размером каталога).\n",
    "\n",
    "Сформируем такую матрицу, используем в качестве индекса строк и столбцов непосредственно значения userid,\tmovieid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7955, 171764)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ui_matrix = coo_matrix((\n",
    "    [row[2] for row in ui_data],\n",
    "    ([row[0] for row in ui_data], [row[1] for row in ui_data])\n",
    ")).astype(np.float16)\n",
    "ui_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашей матрице 16912 строк и 176212 столбцов. Однако там много полностью нулевых строк (id юзеров, которые не попали в выборку) и большое количество нулевых столбцов (id контента, которого не слуществует). Проверим, сколько у нас пустых id контента и и пользователей):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% пользователей без активности 0.4253928346951603\n",
      "% контента без просмотров 0.9723748864721362\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"% пользователей без активности {}\n",
    "% контента без просмотров {}\"\"\".format(\n",
    "        1 - np.unique(ui_matrix.nonzero()[0]).size/ui_matrix.shape[0],\n",
    "        1 - np.unique(ui_matrix.nonzero()[1]).size/ui_matrix.shape[1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: данные хранятся неоптимально, нужно переиндексировать пользователей и контент чтобы избавиться от пустых строк и столбцов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# индекс пользователей\n",
    "user_index = {\n",
    "    i[1]: i[0][0] \n",
    "    for i in np.ndenumerate(np.unique([triplet[0] for triplet in ui_data]))\n",
    "}\n",
    "# обратный индекс - нужен для фронтэнда\n",
    "inverse_user_index = {j: i for i, j in user_index.items()}\n",
    "\n",
    "# аналогично индекс контента\n",
    "item_index = {\n",
    "    i[1]: i[0][0] \n",
    "    for i in np.ndenumerate(np.unique([triplet[1] for triplet in ui_data]))\n",
    "}\n",
    "inverse_item_index = {j: i for i, j in item_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применяем к выгрузке из SQL преобразование индексов и формируем новую user-item матрицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4571, 4745)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raiting_list = [row[2] for row in ui_data]\n",
    "user_index_plain = [user_index[row[0]] for row in ui_data]\n",
    "item_index_plain = [item_index[row[1]] for row in ui_data]\n",
    "\n",
    "ui_matrix = coo_matrix((raiting_list, (user_index_plain, item_index_plain))).astype(np.float16)\n",
    "# del df, ui_data # если не хватит памяти\n",
    "ui_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, размерноть матрицы сильно уменьшилось.\n",
    "\n",
    "Предварительная обработка данных из хранища - обязательный шаг в машинном обучении. Правильная предварительная обработка позволяет экономить вычислительные ресурсы.\n",
    "\n",
    "\n",
    "## Обучаем модель\n",
    "\n",
    "Данные подготовлены для обучения - можем построить модель.\n",
    "\n",
    "В качестве рекомендательной модели мы будем использовать SVD-разложение матрицы user-item. В результате разложения матрица user-item $S$ размерности $m \\times n$ будет представлена в виде произведения двух матриц меньшей размерности - матрицей факторов контента и матрицей факторов пользователей.\n",
    "\n",
    "$$\n",
    "S = U \\times I^T\n",
    "$$\n",
    "\n",
    "При этом матрицы размерности $U\\sim m \\times k$ и $I \\sim m \\times k$, где $m$ и $n$ порядка нескольких тысяч, а $k$ - размерность пространства скрытых факторов, обычно не превышает 100.\n",
    "\n",
    "Подробнее про SVD можно почитать тут https://habr.com/company/surfingbird/blog/139863/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4571, 50) (50, 4745)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "num_users, num_items = ui_matrix.shape\n",
    "user_factors ,scale, item_factors = svds(ui_matrix.asfptype(), k=50, return_singular_vectors=True)\n",
    "#create square matrix\n",
    "scale = np.diag(np.sqrt(scale))\n",
    "user_factors = np.dot(user_factors, scale).astype(np.float16)\n",
    "item_factors = np.dot(scale, item_factors).astype(np.float16)\n",
    "\n",
    "print(user_factors.shape, item_factors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сохраняем модель в MongoDB, Redis\n",
    "\n",
    "Итак, мы:\n",
    "- выгрузили данные из Postgres с помощью запроса SQL\n",
    "- провели небольшую работу по агрегации и очистке данных на стороне SQL\n",
    "- выполнили небольшой процессинг(переиндексацию) в Python.\n",
    "- обучили модель\n",
    "\n",
    "На этапе эксплуатации нам нужно рекомендовать контент для пользователя. Чтобы посчитать персональные рекомендации для каждого пользователя нужно переменожить факторы пользователя на факторы контента, которые мы получили на этапе обучения модели - для этого нужно сохранить эти два массива.\n",
    "\n",
    "В нашем мини-проекте мы применяем Postgres для хранения \"сырых\" данных. Для хранения модели больше подходят нереляционные хранилища данных - им посвящена вторая часть курса. Нереляционные (NoSQL) хранилища обладают двумя главными преимуществами - их просто настраивать и лего масштабировать.\n",
    "\n",
    "Мы будем использовать Mongo для хранения факторов пользователей и Redis как общий кэш для хранения факторов контента\n",
    "\n",
    "Оба хранилища подняты в докере, порты пробрасываются с локальной машины - за общением между сервисами бэкенда можно наблюдать прямо в консоли."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from redis import Redis\n",
    "\n",
    "from lz4.block import compress, decompress\n",
    "from msgpack import packb, unpackb\n",
    "from msgpack_numpy import decode, encode\n",
    "\n",
    "mongo_conf = {'host': \"localhost\", 'port': 27018}\n",
    "mongo_storage = MongoClient(**mongo_conf)\n",
    "mongo_recsys_storage = mongo_storage.get_database(\"recsys\")\n",
    "\n",
    "# инициализируем хранилище Mongo\n",
    "user_factors_storage = mongo_recsys_storage.get_collection(\"user\")\n",
    "\n",
    "# инициализируем хранилище Redis\n",
    "REDIS_CONF = {\"host\": \"localhost\", \"port\": 6380, \"db\": 0}\n",
    "redis_storage = Redis(**REDIS_CONF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим установку - версия должна быть такой же, что и в файле requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymongo\n",
    "pymongo.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы сохраняем факторы пользователей в Mongo: по одному документу на каждого пользователя\n",
    "Факторы контента сохраняем в Redis как один массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сохраняем факторы пользователей\n",
    "selector = {'id': {'$in': [user_id for user_id in range(num_users)]}}\n",
    "user_factors_storage.delete_many(selector)\n",
    "user_factors_storage.insert_many(\n",
    "    [\n",
    "        {\n",
    "            'id': user_id,\n",
    "            'value': compress(packb(user_factors[user_id,:], default=encode))\n",
    "        } \n",
    "        for user_id in range(num_users)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# сохраняем факторы контента\n",
    "redis_storage.set(\"item_factors\" , compress(packb(item_factors, default=encode)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выдачи рекомендаций мы получаем факторы пользователя из Mongo, матрицу факторов контента из Redis и и перемножаем их. Получаем вектор размерности $1\\times n$ - то есть каждому кконтенту из каталога соответствует некое число - т.н. \"релевантность\". Осталосль отсортировать каталог по значению релевантности - контент с самым высоким значением должен понравится пользователю\n",
    "\n",
    "_Примечание_: если хватит памяти на машине, можно предрасчитать рекомендации для всех пользователей сразу как\n",
    "\n",
    "<pre>\n",
    "recs = np.dot(user_factors, item_factors)\n",
    "</pre>\n",
    "\n",
    "_Домашнее задание_: сохранить в Mongo по каждому пользователю предрастчитанное ранжирование (дополнительно к факторам контента). Подсказка: используйте функцию *update* из PyMongo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_factors (50,)\n",
      "item_factors (50, 4745)\n",
      "recommendations (1, 4745)\n"
     ]
    }
   ],
   "source": [
    "# матрица факторов пользователя\n",
    "mongo_doc = user_factors_storage.find_one({'id': 100})\n",
    "if mongo_doc is None:\n",
    "    print(\"Пользователя с id %(id)d не существует\" % user_doc)\n",
    "else:\n",
    "    latent_user_factors = unpackb(decompress(mongo_doc['value']), object_hook=decode)\n",
    "\n",
    "    # матрица факторов контента\n",
    "    redis_data = redis_storage.get(\"item_factors\")\n",
    "    latent_item_factors = unpackb(decompress(redis_data), object_hook=decode)\n",
    "\n",
    "    # вычисляем персональную релевантность контента\n",
    "    personal_recs = latent_user_factors.reshape(1,-1).dot(latent_item_factors)\n",
    "\n",
    "\n",
    "    print(\"user_factors {}\\nitem_factors {}\\nrecommendations {}\".format(\n",
    "            latent_user_factors.shape, latent_item_factors.shape, personal_recs.shape)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили рекомендации в виде массива. На финальном этапе нажно отфильтровать top-100 самых релевантных пользователю единиц контента и выполнить преобразование из нашего плотного индекса обратно к *movieId*\n",
    "\n",
    "_Домашнее задание_: залить в Mongo другую полезную информацию из репозитория и сделать форму вывода более богатой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[524, 1186, 2881, 3363, 3510, 362, 1321, 914, 531, 2145, 1721, 277, 2501, 902, 135, 2366, 2144, 3424, 2707, 3107, 2605, 2572, 719, 2160, 849, 1235, 1663, 830, 1172, 2908, 1680, 3072, 1513, 4002, 3100, 170, 1019, 1378, 2140, 1968, 107, 2427, 3148, 2746, 2371, 2944, 898, 1416, 1957, 1962]\n"
     ]
    }
   ],
   "source": [
    "user_recommendations = [inverse_item_index[i] for i in np.argsort(-personal_recs[0])[:50]]\n",
    "print(user_recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы\n",
    "\n",
    "Мы построили веб-приложение на Flask, которое демонстрирует процесс работы с данными для построения рекомендательной системы\n",
    "\n",
    "- Сырые данные хранятся в баз Postgres\n",
    "- Первичная обработка происходит внутри SQL\n",
    "- Пост-обработка данных и обучения модели происходит на стороне Python\n",
    "- Эксплуатация модели производится с помощью хранилища на Mongo+Redis\n",
    "- Связь между хранилищами данных и фронт-этом осуществляется с помощью Flask\n",
    "\n",
    "В заключение: разрабатывать архитектуру хранения данных для приложения нужно с учётом сильных и слабых сторон каждой технологии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
